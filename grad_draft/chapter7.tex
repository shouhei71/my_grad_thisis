%%%%
\chapter{実験・評価}

\section{予備実験}
本実験に入る前に以下の予備実験を行った。
\subsection{予備実験 (1) gossipの通信量推定の妥当性}
この実験によって、Cassandraノードで発生するgossipの通信量の推定が妥当であることを裏付ける。異なるマシン数で同数のCassandraノードを立ち上げて通信量の推定を行い比較する。予備実験では、120ノードのCassandraをTypeA、TypeBの2パターンで立ち上げ、10分間の計測後に推定される通信量が一致することを確認する。
\begin{figure}[h]
 \begin{center}
  \includegraphics[bb=0 0 504 504, width=\columnwidth]{img/infer-traffic.png}
  \caption{マシン台数に応じたgossipの通信量の時間変化}
  \label{fig:infer-gossip-traffic}
 \end{center}
\end{figure}
推定される通信量を$T$とおく。
\begin{itemize}
\item TypeA\\
一台あたり12ノードのCassandraを立ち上げたマシン10台でのクラスタを構成
\item typeB\\
一台あたり60ノードのCassandraノードを立ち上げたマシン2台でのクラスタを構成
\end{itemize}
Type A の場合は、1秒あたりの通信量を$A(t)$とおくと、式\ref{eq:eq_data-traffic}から
\begin{eqnarray}
T(t)  =  A(t) \times 119 /  110
\end{eqnarray}
Type Bの場合は、時間tの時の通信量を$B(t)$とおくと、同様に式\ref{eq:eq_data-traffic}から
\begin{eqnarray}
T(t)  =  B(t) \times 119 / 60
\end{eqnarray}
となる。
1秒ごとに通信量を取得すると非常に変動が激しく、読み取りにくいデータになってしまう。
そこで10秒ごとに通信量を集計し、1秒あたりの平均を算出し、これを通信量の指標とした。
以後の実験すべてにおいて、1秒あたりの通信量とは、これを意味するものとする。
各通信量の推定値をグラフに表したのが図\ref{fig:infer-gossip-traffic}である。


横軸は時間軸で、縦軸は推定される通信量 (Mbit/s) である。(ただし、$1M =10^6$, $1K=10^3$ である。)
図 \ref{fig:infer-gossip-traffic}からマシン数が2台、10台のときともに、200秒以降は通信量が安定している。安定した後の推定される通信量はほぼ同じであることがわかる。よって、この推定が確からしいことが裏付けられた。
\if0
\subsection{予備実験 (2) :seed数に対するgossip通信量の時間変化}
\begin{figure}[!h]
  \begin{center}
  \includegraphics[bb=0 0 504 450 , width=\columnwidth]{img/seed-traffic.png}
  \caption{seed数に対するgossip通信量の時間変化}
  \label{fig:seed-traffic}
    \end{center}
\end{figure}
また、seedの数が変化したときの通信量の変化を調べた。
実験では、以下のように、seed数を1, 2, 4, 8,120と変化させながら、5台のマシン上でcassandra120台を起動し通信量の変化を観察した。次のグラフが測定した図\ref{fig:seed-traffic}である。
0-100秒の間では、seedの数が60, 120の時だけ通信量が急激に増大しているが、その後はすべて一定の値に収束している。よって、安定時はseedの数によらず通信量が一定だということがわかる。

またseed数が120の時だけ通信量が急激に増大している理由は、Cassandraのメンバーシップ管理のアルゴリズムをみると明らかである。seedの数が現在生存しているノードよりも少ないときは、seedに通信を行う必要がある。このため、各ノードが全ノードを把握するまでgossip通信が多く行われることになる。


\fi
\subsection{予備実験 (2) :データ読み書き通信量推定の妥当性}
\begin{figure}[h]
  \begin{center}
  \includegraphics[bb=0 0 504 450 ,width=\columnwidth]{img/ycsb-machine-traffic.png}
  \caption{マシン台数別のデータ読み書きの通信量推定値}
  \label{fig:ycsb-machine-traffic}
    \end{center}
\end{figure}

予備実験 (1)  gossip通信量推定の妥当性と同様に、Cassandraノードで発生するデータ読み書きの通信量推定が妥当であることを裏付ける。そのために、異なるマシン数で同数のCassandraノードを立ち上げてデータ通信量の推定を行い比較する。ノード数は120と固定して、マシン数を3,4,5と変化させて通信量の推定を行った。1秒あたりのクエリー数 (queries per second, qps) を5000とした。その結果が図 \ref{fig:ycsb-machine-traffic}である。横軸がマシンの台数で縦軸がYCSBのワークロード実行時のデータ通信量の平均 (Mbit/s) を表している。

構成するマシンが3台の時、通信量が42.3 Mbit/s、マシンが4台の時42.6 Mbit/s、マシンが5台の時43.6 Mbit/sである。マシン台数が増えるにつれわずかに通信量も増加しているが、推定値がほぼ同じであることが確かめられる。よってこの推定がもっとももらしいことが裏付けられた。



\subsection{予備実験 (3) :リクエスト量に応じたデータ読み書き通信量の増加}
\begin{figure}[h]
  \begin{center}
  \includegraphics[bb=0 0 504 450 ,width=\columnwidth]{img/ycsb-query-traffic.png}
  \caption{リクエスト量に応じたデータ読み書き通信量の増加}
  \label{fig:ycsb-query-traffic}
    \end{center}
\end{figure}

要求されるリクエストの量 (qps) を変化に応じた、Cassandraノードで発生するデータ通信量 (Mbit/s) の変化を測定する。
qpsが増えると処理するリクエストの量が線形に増加するので、通信量も線形に増加していくことが予想される。
ノード台数は120台と固定し、$qps =  500, 1000, 2000, 3000, 4000$と変化させてデータの読み書き通信量を測定した。図\ref{fig:ycsb-query-traffic}が測定結果である。
縦軸がqpsで、横軸がYCSBのワークロード実行時のデータ総通信量である。

図中の直線は、プロットした点から1次関数でフィッティングしたものである。
$q$を1秒あたりのクエリー発行数 (qps) として得られる関数は、	
\begin{equation}
 [通信量 (bit/s)] = 8727.9 \times q - 348939	
\end{equation}
である。
通信量は1秒あたりのクエリー数に応じてO(n)で増加していくことが確認できる。
予想通りの結果が得られた。

\section{本実験}

\subsection{実験1:gossipの通信量}
スケーラビリティ評価を行う。まず、gossip通信量がノード台数に応じてどのように変化していくかを測定した。
図\ref{fig:time-traffic}は、10 秒あたりのCassandraノード間の平均gossip通信量の時間変化をノード数別に表したグラフである。
(ただし、$1M =10^6$, $1K=10^3$ である。)ノード数によらず、100 秒以降は通信量が安定していることがわかる。
図\ref{fig:node-traffic} は、ノード数と通信量が安定している時の(ここでは、実験開始から 200$\sim$300 秒後とした)1秒あたりの通信量の平均をプロットしたものである。

\subsection{実験2:データ読み書き通信量の測定}
次に、データ通信量がノード数に応じてどのように変化していくかを測定した。
$qps$ ($queries/s$) は1000で一定であり、更新時のデータサイズは1kbyteで固定してある。 
図\ref{fig:ycsb-node-traffic}は、YCSBのワークロードを実行したときにデータ読み書きで発生した通信量 (Mbit/s) の平均をノード台数別に表したグラフである。たたしgossipの通信量は差し引いてある。


\begin{figure}[p]
 \begin{center}
  \includegraphics[bb=0 0 504 504,width=\columnwidth]{img/time-traffic.png}
  \caption{ノード数別のgossip通信量の時間変化}
  \label{fig:time-traffic}
 \end{center}
 
\end{figure}
\begin{figure}[p]
 \begin{center}
  \includegraphics[bb=0 0 504 504,width=\columnwidth]{img/node-traffic.png}
  \caption{ノード数に応じたgossip通信量の増加}
  \label{fig:node-traffic}
 \end{center}
\end{figure}

\begin{figure}[p]
  \begin{center}
  \includegraphics[bb=0 0 504 450 ,width=\columnwidth]{img/ycsb-node-traffic.png}
  \caption{qps一定時のノード数に応じたデータ読み書き通信量の変化}
  \label{fig:ycsb-node-traffic}
    \end{center}
\end{figure}


\newpage

\section{評価}
\subsection{gossipの通信量の見積もり}
図中の曲線は、プロットした点から2次関数でフィッティングしたものである。$n$ をノード数として得られた関数は、
\begin{eqnarray}
 [通信量 (bit) ] = 224.6 \times n^2 + 4314.8 \times n
 \label{eq:gossip_traffic_whole}	
\end{eqnarray}
である。よって、gossipの通信量はO($n^2$)で増加することがわかった。
この関数から、ノード台数をパラメータとしてCassandraのgossipプロトコルで発生しうる全体の通信量を推測することができる。
例えば、$n=1000$のとき、$[通信量] = 229 Mbps$となる。このように、関数を使ってgossipによる通信量が見積もることができる。



\subsection{1ノードあたりの通信量}
同様に、1ノードあたりの通信量を見積もることも可能である。
ここでは受信する通信量のみを考えるが、受信する通信量と送信する通信量は等しい。
総通信量をCassandraノード
台数$n$で割った値が、1ノードあたりの通信量となる。
つまり、1ノードあたりの通信量をT'として
\begin{eqnarray}
	T'&=&  (224.6 \times n^2 + 4314.8 \times n) / n \\
	&=& 224.6 \times n+ 4314.8
\end{eqnarray}
とO(n)で増加することがわかる。
%グラフに示すと以下になる。
%縦軸がノード台数、横軸が通信量である。また、この通信量はメンバーシップ管理で受信、送信する通信量のそれぞれの値である。

\subsection{複数のデータセンターをまたぐクラスタにおけるgossipプロトコルの問題点}
\begin{figure}[h]
 \begin{center}
  \includegraphics[bb=0 0 720 440,width=\columnwidth]{img/link_b_dc.png}
  \caption{二つのデータセンターをまたぐクラスタの構成}
  \label{fig:link_b_dc}
 \end{center}
 \end{figure}

ケースとして2つのデータセンターをまたいでクラスタを構成することを考える。 (図\ref{fig:link_b_dc})このクラスタで発生する通信量を見積もる。


このようにデータセンターを複数またいでクラスタを構成することは特別なことではない。
データの複製を別のデータセンターに保持しておくことで、データセンター全体で故障が起きた場合でもデータの損失が避けられる。
このように最近のクラウドストレージではデータセンターの故障にも耐性があるシステムを実現しているのである。

データセンター間を結ぶリンクで発生する通信量を見積もる。
簡単のためにCassandraのノード数を$n$とした時、各データセンターA、Bにそれぞれ$n/2$ノードが起動しているとする。
この時、データセンター間での通信量$T^{A\-B}$は、システム全体で発生する通信量を$T$とおくと、$T/4$である。
一方、式\ref{eq:gossip_traffic_whole}より$T$はO($n^2$)で増加するから$TA\-B$もO($n^2$)で増加する。

このように、クラスタを構成する各ノードの通信量はO($n$)で増加していく一方で、データセンター間を結ぶリンクの通信量はO($n^2$)で増加していく。
つまり、ノード数が増加したときにこのリンク部分の通信が圧迫される可能性がある。
現状のgossipプロトコルには、このようなデータセンター間のリンク部分を考慮したアルゴリズムではないことがわかる。

データセンターをまたいでクラスタを構成することが主流の現在において、リンク部分の通信量を考慮してgossipプロコルを応用することが望まれる。例えば、あるノードの故障情報をデータセンターをまたいで伝達させたいとき、現状のgossipプロトコルでは同じ情報を冗長に伝達することが多い。データセンターをまたぐお内内容の通信を取りまとめることができれば、データセンター間のリンクで発生する通信を削減できるだろう。






\if0
・複数(2)データセンターを考える。
・データセンタをつなぐリンクは$n^2$で通信量が増加していく。
・リンク部分は圧迫している
・gossipプロトコルはリンク部分を考慮していないので圧迫する。
・リンクを考慮したgossipプロトコルの提案が望まれる。
・例えば、データの削除
同じノードの故障情報はまとめることができないか？


上より総通信量はある関数に沿って、増加していることがわかった。この結果は、クラスタ設計時に活かすことができる。つまり、クラスタを構成するノード数に応じて、どの機器でどれくらいの通信量が発生するかを見積もることができる。一般的には議論するのは難しいので、クラスタの構成ごとに具体的なケースに焦点を当てて見ていく。

\subsubsection{TYPEA:データセンター3つでクラスタを構成する場合}
\subsubsection{TYPEB:データセンター1つでクラスタを構成する場合}
\subsubsection{TYPEC:ネットワーク1000base-Tのような切り口}
\fi

\subsubsection{gossipの通信量がO($n^2$)で増加していく理由}
gossipの通信量がO($n^2$)で増加していく理由について考察する。
総通信量$T$($bit/s$)は、1ノードあたりの通信量($bit/s$)を$T'$、ノード数を$n$として、
 \begin{eqnarray}
  T = T' \times n 
  \label{eq:t_times_n}
 \end{eqnarray}
である。さらに1秒毎のgossip通信が行われる回数を$s$、1回のgossip通信にかかる通信量$t_{gossip}$($bit$)として、1ノードあたりの通信量$T'$($bit/s$)は、
\begin{eqnarray}
 T' &=&  s \times t_{gossip}
\end{eqnarray}
となる。メンバー構成が安定した時を考える。Cassandraのメンバー管理シップでは、メンバー構成が安定したときは、毎秒1$\sim$2回のgossip通信が行われる。
よって1秒毎のgossip通信する回数$s$は、
\begin{eqnarray}
 s < 2 = O(1)
\end{eqnarray}
 一方、1回あたりのgossip通信の通信量$t_{gossip}(bit)$はScuttlebuttのアルゴリズムから安定時にはノード数に依存するので、
  \begin{equation}
   t_{gossip} = O(n)
    \end{equation}
よって式\ref{eq:t_times_n}より、1ノードあたりの通信量$T'(bit/s)$、 総通信量$T(bit/s)$は、
\begin{eqnarray}
T' &=& O(n)\\
T &=& O(n^2)
\end{eqnarray}
となることが証明できた。
\if0
また上記にCassandra固有の方式はほとんどない。よって、メンバー構成が安定した時のgossip baseのメンバーシップ管理アルゴリズムの通信量のスケーラビリティが評価できたことになる。
\fi

\subsubsection{qps一定時にデータ読み書きの通信量は一定}
$qps$を一定にしたとき、構成するCassandraのノード数$n$に応じて、通信量の増加は緩やかになり一定の値に近づいていくことが確認できる。 (図\ref{fig:ycsb-node-traffic}から) 
これは直感的に明らかである。ノード数が$n$の時、クライアントとやり取りを行うproxyノードがデータの担当ノードである確率は$1/n$である。
つまり、クライアントが要求するデータに対して、$(n-1)/n$の確率で通信が発生することになる。よって、$n$が大きくなるとその確率は1に収束してのでほぼ必ずリクエストに対して通信が発生する。
ノード台数$n$が増加すると、通信量はある一定値に近づいていく。
実際、図\ref{fig:ycsb-node-traffic}も一定値に近づいていくことが見てとれる。
\if0
\subsubsection{gossipによる故障検知はスケールしにくい}
ネットワーク帯域幅が制限されている状況を考える。qpsを一定したとき、このときノード数を増やしていくと、 (ただしnは十分に大きいとすると) データ読み書きのための通信量は図\ref{fig:ycsb-node-traffic}より変わらない一方で、gossipによる通信はO($n^2$)で増加していく。gossipによる通信がスケーラビリティを制約することを表している。
\fi


\if0
% 表の例
\begin{table}[tb]
 \begin{center}
  \caption{Addistant 2の実行時間(秒)}
  \label{tab:time}
  \begin{tabular}{lr}
   \hline
                   & 時間 \\
   \hline
   X Window System & 15.0 \\
   Addistant 1     &  3.0 \\
   Addistant 2     &  2.0 \\
   \hline
  \end{tabular}
 \end{center}
\end{table}

表~\ref{tab:time}は...
\fi
